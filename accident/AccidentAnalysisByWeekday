val baseDir = "./"
val accidentsFile = baseDir + "dftRoadSafetyData_Accidents_2018.csv"
val weatherFile = baseDir + "week.csv"
//如果在csv第一行有属性的话，没有就是"false";自动推断属性列的数据类型;文件的路径
val accidentsDF = spark.read.option("header", "true").option("inferSchema", "true").csv(accidentsFile)
val weatherDF = spark.read.option("header", "true").csv(weatherFile) //读取文件方法与上一条类似

//过滤出ccidentsDF文件中Accident_Severity>1的字段
val results = accidentsDF.filter(accidentsDF.col("Accident_Severity") > 1)
//选取Accident_Index字段的前四个符号并重命名为year，即年份，以及Weather_Conditions字段
val results1=results.select(col("Accident_Index").substr(0, 4).as("year"), col("Day_of_Week"))
//根据year和Weather_Conditions进行分组
val results2=results1.groupBy("year", "Day_of_Week")
//统计组内数据的个数
val results3=results2.count()
//将筛选出来的数据表与weather表进行连接
val results4=results3.join(weatherDF, weatherDF.col("code") === accidentsDF.col("Day_of_Week"))
//选出连接之后的year，label以及count字段
val results5=results4.select(col("year"), col("label").as("week"), col("count"))
//根据count字段进行降序排序
val results6 = results5.orderBy(col("count").desc)
results6.show(truncate = false)